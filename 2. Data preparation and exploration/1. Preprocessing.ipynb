{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cagutierrezgu/My_Portfolio/blob/main/Podcast%20topic%20classification/Data%20preparation%20and%20exploration/Preprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "97b70adf",
      "metadata": {
        "id": "97b70adf"
      },
      "source": [
        "## 3. Preprocesamiento\n",
        "----\n",
        "Al tener los conjuntos de textos cargados de las respectivas páginas web es necesario hacer una limpieza de las palabras antes de implementar cualquier modelo o hacer algún análisis. Algunas de las acciones de limpieza que pueden realizarse consisten en eliminar caracteres especiales, links, stop words o palabras de corta longitud no deseadas para el análisis:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2f50a7cf",
      "metadata": {
        "id": "2f50a7cf"
      },
      "outputs": [],
      "source": [
        "def cleanResume(resumeText):\n",
        "    \"\"\"Elimina caracteres y palabras indeseadas en el texto\"\"\"\n",
        "    resumeText = re.sub('http\\S+\\s*', ' ', resumeText)  # elimina URLs\n",
        "    resumeText = re.sub('RT|cc', ' ', resumeText)  # elimina RT and cc\n",
        "    resumeText = re.sub('#\\S+', '', resumeText)  # elimina hashtags\n",
        "    resumeText = re.sub('@\\S+', '  ', resumeText)  # elimina menciones\n",
        "    resumeText = re.sub('[%s]' % re.escape(\"\"\"!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\"\"\"), ' ', resumeText)  # elimina puntuaciones\n",
        "    resumeText = re.sub(r'[^\\x00-\\x7f]',r' ', resumeText) \n",
        "    resumeText = re.sub('\\s+', ' ', resumeText)  # elimina espacios en blanco extras\n",
        "    return resumeText\n",
        "\n",
        "def stop_words(text):\n",
        "    \"\"\"Remueve stop words y palabras cortas\"\"\"\n",
        "    filtered = []\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    word_tokens = word_tokenize(text)\n",
        "    for w in word_tokens:\n",
        "        if w not in stop_words and w.isalpha():\n",
        "            if len(w)>3:\n",
        "                filtered.append(w.lower())\n",
        "    return filtered"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4f9ee497",
      "metadata": {
        "id": "4f9ee497"
      },
      "source": [
        "Al aplicar las anteriores funciones sobre cada una de las listas con los textos disponibles, se tiene:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a1feae30",
      "metadata": {
        "id": "a1feae30"
      },
      "outputs": [],
      "source": [
        "text_science = list(map(cleanResume, text_science))\n",
        "text_sports = list(map(cleanResume, text_sports))\n",
        "text_hf = list(map(cleanResume, text_hf))\n",
        "text_history = list(map(cleanResume, text_history))\n",
        "text_crime = list(map(cleanResume, text_crime))\n",
        "\n",
        "#text_hf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "38cb48e0",
      "metadata": {
        "id": "38cb48e0"
      },
      "outputs": [],
      "source": [
        "tock_science = list(map(stop_words, text_science))\n",
        "tock_sports = list(map(stop_words, text_sports))\n",
        "tock_hf = list(map(stop_words, text_hf))\n",
        "tock_history = list(map(stop_words, text_history))\n",
        "tock_crime = list(map(stop_words, text_crime))\n",
        "\n",
        "#tock_history"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "71e1fab6",
      "metadata": {
        "id": "71e1fab6"
      },
      "source": [
        "Por otro lado, resultará útil para posteriores tratamientos aplicar lemmatization sobre los textos, luego:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "920730a9",
      "metadata": {
        "id": "920730a9"
      },
      "outputs": [],
      "source": [
        "def lemmatization(text):\n",
        "    wlem = WordNetLemmatizer()\n",
        "    word_tokens = []\n",
        "    for i in text:\n",
        "        word_tokens.append(wlem.lemmatize(i))\n",
        "    return word_tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d079ef9d",
      "metadata": {
        "id": "d079ef9d"
      },
      "outputs": [],
      "source": [
        "tock_science = list(map(lemmatization, tock_science))\n",
        "tock_sports = list(map(lemmatization, tock_sports))\n",
        "tock_hf = list(map(lemmatization, tock_hf))\n",
        "tock_history = list(map(lemmatization, tock_history))\n",
        "tock_crime = list(map(lemmatization, tock_crime))\n",
        "\n",
        "#tock_crime"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}